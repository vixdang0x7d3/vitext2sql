import os
import pandas as pd
from tqdm import tqdm
import argparse
import shutil
import sqlite3
from utils import remove_digits, is_file, clear_description, clear_sample_rows, extract_column_names, get_api_name, clear_name, remove_declare_lines, clear_byte
import json

def process_ddl(ddl_file):
    table_names = ddl_file['table_name'].to_list()
    representatives = {}
    for i in range(len(ddl_file)):
        if remove_digits(table_names[i]) in representatives.keys():
            representatives[remove_digits(table_names[i])] += [table_names[i]]
        else:
            representatives[remove_digits(table_names[i])] = [table_names[i]]

    for i in range(len(ddl_file)):
        if remove_digits(table_names[i]) in representatives:
            if len(representatives[remove_digits(table_names[i])]) > 10:
                if ddl_file['table_name'][i] != representatives[remove_digits(table_names[i])][0]:
                    ddl_file = ddl_file.drop(index=i)
            else:
                # representatives[table_names[i]] = [table_names[i]]
                del representatives[remove_digits(table_names[i])]
    return ddl_file, representatives
def compress_ddl(dialect = "sqlite",db_name="",id=1,add_description=False, add_sample_rows=False, rm_digits=False, schema_linked=False, clear_long_eg_des=False, sqlite_sl_path=None, reduce_col=False, use_gold_table=False, use_gold_schema=False):
    print("Compress DDL files.")
    db_folder = os.path.join("db", db_name)
    
    schema_path = os.path.join(db_folder,"schema")

    external_knowledge = None
    prompts = ''
    if  dialect == "sqlite":
        table_dict = {}
        representatives = None
        schema_name_path=os.path.join(schema_path,"DDL.csv")
        ddl_sl_flag = False
        if schema_linked:
            temp_file = os.path.join(db_folder, "ddl_sl")
            if os.path.exists(os.path.join(temp_file,str(id) + ".csv")):
                ddl_sl_flag = True
                schema_name_path = os.path.join(temp_file,str(id) + ".csv")
        if not os.path.exists(schema_name_path):
            print(f"Không tìm thấy file schema: {schema_name_path}")
            print("Đường dẫn tuyệt đối:", os.path.abspath(schema_name_path))
            print("File tồn tại?", os.path.exists(schema_name_path))
            # print("Danh sách file trong folder schema:", os.listdir(os.path.dirname(schema_name_path)))
        ddl_file = pd.read_csv(schema_name_path)

        
        if schema_linked and len(ddl_file['table_name'].to_list()) < 10:
            pass
        elif rm_digits:
            ddl_file, representatives = process_ddl(ddl_file)
        table_name_list = ddl_file['table_name'].to_list()
        ddl_file.reset_index(drop=True, inplace=True)
        db_name_path=schema_path
        for i in range(len(table_name_list)):
            if os.path.exists(os.path.join(db_name_path, table_name_list[i]+".json")):                               
                with open(os.path.join(db_name_path, table_name_list[i]+".json"),encoding="utf-8") as f:
                    table_json = json.load(f)
            else:
                print(f"No table: {os.path.join(db_name_path, table_name_list[i])}")
                continue
            
            prompts += "Table full name: " + table_json["table_fullname"] + "\n"

            if reduce_col and ddl_sl_flag:
                assert schema_linked
                full_name = table_json["table_fullname"]
                short_name = full_name.split(".")[-1].strip()

                ddl_file.columns = ddl_file.columns.str.strip().str.lower()
                ddl_file["table_name"] = ddl_file["table_name"].str.strip()
                matched = ddl_file[ddl_file["table_name"] == short_name].iloc[0]
                
                col_names = matched["ddl"]

            if "primary_keys" in table_json and table_json["primary_keys"]:
                prompts += "Primary keys:\n"
                for pk in table_json["primary_keys"]:
                    prompts += f"    {pk}\n"

            # Ghi thông tin khóa ngoại
            if "foreign_keys" in table_json and table_json["foreign_keys"]:
                prompts += "Foreign keys:\n"
                for fk in table_json["foreign_keys"]:
                    prompts += f"    {fk['from_column']} → {fk['to_table']}.{fk['to_column']}\n"

            # Ghi thông tin is_referenced_by
            if "is_referenced_by" in table_json and table_json["is_referenced_by"]:
                prompts += "Is referenced by:\n"
                for ref in table_json["is_referenced_by"]:
                    prompts += f"    {ref['from_table']}.{ref['from_column']}\n"

            column_prefix = "column_"
            for j in range(len(table_json[f"{column_prefix}names"])):
                table_des = ''
                if add_description:
                    if j < len(table_json["description"]):
                        table_des = " Description: " + str(table_json["description"][j]) if table_json["description"][j] else ""
                    elif table_json[f"column_names"][j] != "_PARTITIONTIME":
                        print(f" description unmatch {table_name_list[i]}")

                if reduce_col and ddl_sl_flag:
                    if table_json[f"{column_prefix}names"][j] in col_names:
                        prompts += "Column name: " + table_json[f"{column_prefix}names"][j] + " Type: " + table_json[f"{column_prefix}types"][j] + table_des +"\n"
                else:
                    prompts += "Column name: " + table_json[f"{column_prefix}names"][j] + " Type: " + table_json[f"{column_prefix}types"][j] + table_des +"\n"
            if add_sample_rows:                                            
                if reduce_col and ddl_sl_flag:
                    sample_rows = [{col: row[col] for col in extract_column_names(col_names) if col in row} for row in table_json["sample_rows"]]
                else:
                    sample_rows = table_json["sample_rows"]
                sample_rows = clear_byte(sample_rows)
                prompts += "Sample rows:\n" + str(sample_rows) + "\n"
            if representatives is not None:
                if remove_digits(table_name_list[i]) in representatives:
                    if len(representatives[remove_digits(table_name_list[i])]) > 1:
                        assert len(representatives[remove_digits(table_name_list[i])]) >= 10, representatives[remove_digits(table_name_list[i])]
                        prompts += f"Some other tables have the similar structure: {representatives[remove_digits(table_name_list[i])]}\n"
            prompts += "\n" + "-" * 50 + "\n"
    
    if reduce_col :
        output_path=os.path.join(db_folder, "final_context_prompts")
        os.makedirs(output_path, exist_ok=True)
        
        with open(os.path.join(output_path,str(id)+".txt"), "w",encoding="utf-8") as f:
            prompts = clear_sample_rows(prompts, byte_limit=1000)
            if len(prompts) > 200000 and clear_long_eg_des:
                prompts = clear_description(prompts)
            
            external_knowledge =  os.path.join(db_folder, "external_knowledge")
    
            output_sql_ex_file = os.path.join(external_knowledge, "sql_ex_context")
 
            output_db_des_file = os.path.join(external_knowledge, "db_des_context")

            with open(os.path.join(output_sql_ex_file, str(id)+".txt"), encoding="utf-8") as a:
                external_knowledge = a.read()
            prompts += f"External knowledge that might be helpful: \n{external_knowledge}\n"
            with open(os.path.join(output_db_des_file, str(id)+".txt"), encoding="utf-8") as a:
                external_knowledge = a.read()
            prompts +=f"\n{external_knowledge}\n"
            f.writelines(prompts)
    else:       
           
        output_path=os.path.join(db_folder, "prompts")
        os.makedirs(output_path, exist_ok=True)
        with open(os.path.join(output_path,str(id)+".txt"), "w",encoding="utf-8") as f:
            prompts = clear_sample_rows(prompts, byte_limit=1000)
            if len(prompts) > 200000 and clear_long_eg_des:
                prompts = clear_description(prompts)

            # with open(r"D:\RAG_Vitext2sql\db_context.txt", encoding="utf-8") as a:
            #     external_knowledge = a.read()
            # external_knowledge =  os.path.join(db_folder, "external_knowledge")
 
            # output_db_des_file = os.path.join(external_knowledge, "db_des_context")

            # with open(os.path.join(output_db_des_file, str(id)+".txt"), encoding="utf-8") as a:
            #     external_knowledge = a.read()
            # prompts += f"External knowledge that might be helpful: \n{external_knowledge}\n"
            
            f.writelines(prompts)
            

# compress_ddl(db_name="baseball_1",add_description=True, add_sample_rows=True, rm_digits=True, schema_linked=False, clear_long_eg_des=True)
